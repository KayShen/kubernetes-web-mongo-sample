# Set the project
gcloud config set project test-cg

# Get the credentials
gcloud container clusters get-credentials minefield

# Next, to set a zone or region property in the client, run:
gcloud config set compute/zone ZONE
gcloud config set compute/region REGION
gcloud config list

# Check the current context
kubectl config current-context

# Create the persistent disks
gcloud compute disks create --size 200GB mongodb-pd-01
gcloud compute disks create --size 200GB mongodb-pd-02
gcloud compute disks create --size 200GB mongodb-pd-03 --project=test-cg --zone=europe-west1-c

# Create the namespace
kubectl create -f k8s/minefield.namespace.yaml
# Check all namespaces
kubectl get namespace

####################
#### Mongo Part ####
####################

# Create the services
kubectl create --namespace=minefield -f k8s/mongodb-1.service.yaml
kubectl create --namespace=minefield -f k8s/mongodb-2.service.yaml
kubectl create --namespace=minefield -f k8s/mongodb-3.service.yaml

# Create the replication controllers
kubectl --namespace=minefield create -f k8s/mongodb-1.rc.yaml
kubectl --namespace=minefield create -f k8s/mongodb-2.rc.yaml
kubectl --namespace=minefield create -f k8s/mongodb-3.rc.yaml

# Connect to one pod and setup the mongo replication set
kubectl exec -it --namespace=minefield mongodb-1-##### /bin/bash

# Connect to mongo from within the pod
mongo
# Check the Replica Set status
rs.status()
# Initialize the Repelica Set
rs.initiate()
# See the status again
rs.status()
# Change the configuration names
conf = rs.conf()
conf.members[0].host = "mongodb-1:27017"
rs.reconfig(conf)
# Add the other two services
rs.add("mongodb-2:27017")
rs.add("mongodb-3:27017")
# Check the cluster stauts again to ensure that we have 1 PRIMARY and 2 SECONDARY MongoDB instances
rs.status()
# Connect to another mongo instance to see if everything is the same. Note the port number
mongo mongodb-2:27017 

#######################
#### Frontend part ####
#######################

# Create the minefield-guestbook service
kubectl create --namespace=minefield -f k8s/server-minefield.service.yaml

# Create the minefield-guestbook replication controller
kubectl create --namespace=minefield -f k8s/server-minefield.rc.yaml


# Create the Load Balancer service
kubectl create --namespace=minefield -f k8s/server-minefield.service-loadbalancer.yaml


# Create the ingress object - for accessing the service online
kubectl create --namespace=minefield -f k8s/letsencryptor.ingress.yaml


# Get wide output of all pods - useful to see how they are distributed among instances
# PROBLEMS TO BE FIGURED OUT!
kubectl get pods -o wide --namespace=minefield



----------------------------------------------------------------------------------------------------

# Monitoring commands
watch "kubectl config current-context"
watch "kubectl get pod --namespace=minefield"
watch "kubectl get rc --namespace=minefield"
watch "kubectl get service --namespace=minefield"
watch "kubectl get ingress --namespace=minefield"

watch kubectl get service --all-namespaces


# For debugging
kubectl describe pod --namespace=minefield
kubectl get events --namespace=minefield

# Getting the logs
kubectl logs --namespace=minefield mongodb-1-jjmi4

# Deleting stuff
kubectl delete rc mongodb-1 --namespace=minefield

# Other useful commands
kubectl get nodes
kubectl describe nodes

# Docker save built image to tar file
docker save -o nginxplus.tar nginxplus
# Docker load a tar image
docker load -i nginxplus.tar



----------------------------------------------------------------------------------------------------
Questions:
- What will happen if I delete one pod?
- How about if that pod is using a PD?

----------------------------------------------------------------------------------------------------